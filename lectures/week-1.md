# Section 1 Notes

## Problem Solving Process

1. Identify data that is relevant to the problem.
2. Assemble a set of data related to the problem you're trying to fix.
3. Decide on the type of output you are predicting.
4. Based on the type of out,pick an algorithm that will determine a correlation between your 'features' and 'labels'.
5. Use model generated by algorithm to make a prediction.

Decided on the type of output.
Classification -> The value of our labels belongs to a discrete set. The value of our labels belong to a discrete set.

Regression -> The value of our labels belong to a continuous set. We are trying to predict the value of our sets. The value of our labels belong to a continuous set.

## Problem Solving Process:

![Problem Solving Process](img/week-1/week-1-problem-solve-process.png "Problem Solving Process")

1. Identify the data:
Game Breakdown
![Game Breakdown](img/week-1/week-1-game-breakdown.png
 "Game Breakdown")
 Using an array to capture the data.
 ![Array of Arrays](img/week-1/04-array-of-arrarys-used.png)
 Since the ball land in a designated buckets(labels/categories) the type of output selected will be classification.[steps 1 to 3]

4. Based on the data the algorithm selected was K-Nearest Neighbor(knn) Idiom that relates to knn is birds of a feather flock together

## K-Nearest Neighbor (KNN)

![K-Nearest Neighbor](img/week-1/05-knn-problem.png "K-Nearest Neighbor")

After building the first algorithm you may not see the results you want.
![Bad Predictions](img/week-1/06-bad-predictions.png "Bad Predictions")

## Multi Dimensional KNN Algorithm

Using the Pythagorean theorem to calculate the distance between each ball.
Two feature distance calculation (bounciness and drop location). Adding the ball size will make this a three feature calculation.

![Pythagorean theorem](img/week-1/07-distance-ball.png)
![3D Pythagorean theorem](img/week-1/08-3d-pt.png)

End of fundamental of introduction.
![End of intro](img/week-1/09-end-of-intro.png)

## Pros and Cons of Lodash

![Pros and cons of lodash](img/week-1/10-pros-cons-lodash.png)

## Pros and Cons of Tensorflow JS

Methods are very similar to Lodash making the transition from Lodash very easy.
![Tensorflow JS](img/week-1/11-tensorflow-pro-con.png)

## Tensorflow

You can find Tensorflow docs at: [Docs](https://js.tensorflow.org)

## Tensor dimensions

Dimensions in an array.
![Tensor dimensions](img/week-1/12-dim.png)

3D Tensor dimensions.
![3D Tensor](img/week-1/13-3d-dims.png)

## Shape property

Row - Columns
![row columns](img/week-1/14-2d-shapes.png)

## Linear Regression (different paradigm than knn)

The goal of linear progression is to find an independent variables we can relay to a dependent variable. One great benefit of using Tensor is having one or more independent variables to one dependant variable. This is a major benefit than spreadsheet linear tables which only allow the use of one independent variable to one dependant variable.
![Pros and cons of Linear Regression](img/week-1/15-linear-regression.png)

  

## Mean Squared Error

Using the two mean squared error we can calculate how wrong were we from the actual data points. 

![Mean Squared Error](img/week-1/16-mean-sq-error.png)
**The lower the mean squared error the close it is to the actual data point. MSE is unlikely to ever be exactly 0.** For example in the chart below even the close guesses they still have a gap from the equation line.

![MSE Zero](img/week-1/17-mse-zero.png)

![MSE EQ](img/week-1/17-mse-eq.png)

## Derivatives

A method of determining the slope of the MSE. So to get the derivative of y=x2+5 get 2x. We can use that find out where we are on the MSE slope.

![Derivative](img/week-1/18-deriva.png)

![MSE Slope](img/week-1/19-mse-slope.png)

## Gradient Descent Algorithm

![Gradient Descent](img/week-1/20-gradient-algo.png)

## Common Questions

Why the learning rate?

The learning rate allows you to incrementally guess b without over shooting the optimal value. For example if you change the learning rate from .4 to 2 the second guess is 988 completely overshooting 245(optimal). Essential we only use the learning rate to tame the adjustments to b without overshooting over the optimal setting (245 in this case.). Some project require a learning rate of .00002 and other will require a learning rate of 10, this is dependent on the data set you are working with.

Why worry about derivatives?
Calculating the slope allows us to calculate b and mse in one calculation.

## Gradient Descent equation update to include M

Gradient Descent with multiple terms:
Initially we were solving for b and set M=0 to make things easier. Now we are also solving for M. The update will now be ((0x + b) - Actual) to ((mx + b) - Actual). The idea is the same, but now we will be doubling the calculations. After the change, we will now be calculating for B and M, instead of just B.

![Eq update with M](img/week-1/21-eq-update-with-m.png)

##